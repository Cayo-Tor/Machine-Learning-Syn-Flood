{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3161fdf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col \u001b[38;5;66;03m#To drop stuff\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#Data into Spark\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#spark = SparkSession.builder.appName('BOUN_TCP_Anon').getOrCreate()\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m15g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBOUN_TCP_Anon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[0;32m     29\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCayo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms5117135_Project_Code\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms5117135_Project_Code\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBOUN_TCP_Format.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 392\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 339\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    111\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.types import StructType, StructField, NumericType\n",
    "from pyspark.sql.functions import col #To drop stuff\n",
    "\n",
    "#Data into Spark\n",
    "spark = SparkSession.builder     .master('local[*]')     .config(\"spark.driver.memory\", \"15g\")     .appName('BOUN_TCP_Anon')     .getOrCreate()\n",
    "spark_df = spark.read.csv(\n",
    "    path=r'C:\\Users\\Cayo\\Downloads\\s5117135_Project_Code\\s5117135_Project_Code\\BOUN_TCP_Format.csv',\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8694b15e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Index all IP address with unique INT value from String in new column\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m indexer_dst \u001b[38;5;241m=\u001b[39m \u001b[43mStringIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDst_IP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDst_IP_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m indexer_model \u001b[38;5;241m=\u001b[39m indexer_dst\u001b[38;5;241m.\u001b[39mfit(spark_df)\n\u001b[0;32m      8\u001b[0m indexed_data_dst\u001b[38;5;241m=\u001b[39m indexer_model\u001b[38;5;241m.\u001b[39mtransform(spark_df)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\__init__.py:114\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\feature.py:3711\u001b[0m, in \u001b[0;36mStringIndexer.__init__\u001b[1;34m(self, inputCol, outputCol, inputCols, outputCols, handleInvalid, stringOrderType)\u001b[0m\n\u001b[0;32m   3706\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3707\u001b[0m \u001b[38;5;124;03m__init__(self, \\\\*, inputCol=None, outputCol=None, inputCols=None, outputCols=None, \\\u001b[39;00m\n\u001b[0;32m   3708\u001b[0m \u001b[38;5;124;03m         handleInvalid=\"error\", stringOrderType=\"frequencyDesc\")\u001b[39;00m\n\u001b[0;32m   3709\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3710\u001b[0m \u001b[38;5;28msuper\u001b[39m(StringIndexer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m-> 3711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.ml.feature.StringIndexer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3712\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m   3713\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:62\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03mReturns a new Java object.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m---> 62\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     64\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\util.py:38\u001b[0m, in \u001b[0;36m_jvm\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvm\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load _jvm from SparkContext. Is SparkContext initialized?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "#Index all IP address with unique INT value from String in new column\n",
    "indexer_dst = StringIndexer(inputCol=\"Dst_IP\", outputCol=\"Dst_IP_index\")\n",
    "indexer_model = indexer_dst.fit(spark_df)\n",
    "indexed_data_dst= indexer_model.transform(spark_df)\n",
    "\n",
    "indexer_src = StringIndexer(inputCol=\"Src_ip\", outputCol=\"Src_IP_index\")\n",
    "indexer_model_src = indexer_src.fit(indexed_data_dst)\n",
    "indexed_data_src = indexer_model_src.transform(indexed_data_dst)\n",
    "\n",
    "indexer_ttl = StringIndexer(inputCol=\"TTL\", outputCol=\"TTL_index\")\n",
    "indexer_model_ttl = indexer_ttl.fit(indexed_data_src)\n",
    "indexed_data = indexer_model_ttl.transform(indexed_data_src)\n",
    "\n",
    "#INDEX value of victim IP 10.50.199.86 is == 0\n",
    "indexed_data.filter(col('Dst_ip').isin(['10.50.199.86']) == True).show(5)\n",
    "indexed_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop column\n",
    "#'ACK', 'TTL_index', 'Dst_Port'\n",
    "\n",
    "index_col_drop=['Time','Frame_No', 'Src_Port', 'Src_IP_index', 'Pro', 'RST', 'Src_ip', 'Dst_IP', 'TTL']\n",
    "indexed_data = indexed_data.drop(*index_col_drop)\n",
    "indexed_data.show(5)\n",
    "indexed_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create features from data\n",
    "#Vectorizes data columns into vecotr column\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "spark_df.columns\n",
    "\n",
    "features_columns = ('SYN','Frame_lng', 'Dst_IP_index', 'TTL_index', 'ACK', 'Dst_Port') #The columns to vecotrize intro features collumn\n",
    "\n",
    "assemble=VectorAssembler(inputCols=features_columns, outputCol='features') #Vectorize\n",
    "\n",
    "assembled_data=assemble.transform(indexed_data) #Dataframe with vectorized column\n",
    "assembled_data.show(5)\n",
    "print(assemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize data\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler # min=0, max=1, (Parameters for scaling)\n",
    "\n",
    "scaler=StandardScaler(inputCol='features',outputCol='standardized')\n",
    "scaler_model=scaler.fit(assembled_data)\n",
    "scaled_data =scaler_model.transform(assembled_data)\n",
    "scaled_data.show(5)\n",
    "scaled_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e54d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS PUTS STANDARDIZED DATA BACK INTO THE COLUMNS FROM ARRAY\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.mllib.linalg import Vectors as OldVectors\n",
    "from pyspark.sql.functions import explode, explode_outer\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#SELECT STANDARIZED COLUMN\n",
    "spkScaled = scaled_data.select(vector_to_array('standardized').alias('standardized'))\n",
    "spkScaled.collect()\n",
    "spkScaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb771f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D object to be read by PCA\n",
    "stand_list = list(spkScaled.select('standardized').toPandas()['standardized'])\n",
    "stand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46f555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compress data into principle componenets\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(stand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1144ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data variance on graph to find optimal PCA value\n",
    "\n",
    "plt.figure(figsize = (9,10))\n",
    "plt.plot(range(1,7), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07201c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose 4 because it is over 80% on the graph\n",
    "pca = PCA(n_components = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ef7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(stand_list) #Standardized list\n",
    "pca.transform(stand_list) #Standardized list\n",
    "scores_pca = pca.transform(stand_list)\n",
    "scores_pca #PCA Compnoents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "\n",
    "for i in range(1,21):\n",
    "    kmeans_pca = KMeans(n_clusters = i, init = 'k-means++', random_state = 42) #random state = reproducable result\n",
    "    kmeans_pca.fit(scores_pca)\n",
    "    wcss.append(kmeans_pca.inertia_)\n",
    "print(wcss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b26dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The elbow method identified 6 clusters as the optimal number.\n",
    "#Silhoutte score said 4 = Silhouette Score: 0.8828039578166229\n",
    "#Same random state as before\n",
    "kmeans_pca = KMeans(n_clusters = 6, init = 'k-means++', random_state = 42)\n",
    "\n",
    "#Fit data to Kmeans model\n",
    "kmeans_pca.fit(scores_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUTTING ALL PCA AND KMEANS LABELS INTO LIST READY TO BE ADDED TO DATAFRAME\n",
    "col1 = []\n",
    "for i in range(0, len(scores_pca)):\n",
    "    a = scores_pca[i][0]\n",
    "    col1.append(a)   \n",
    "\n",
    "col2 = []\n",
    "for i in range(0, len(scores_pca)):\n",
    "    b = scores_pca[i][1]\n",
    "    col2.append(b)  \n",
    "\n",
    "col3 = []\n",
    "for i in range(0, len(scores_pca)):\n",
    "    c = scores_pca[i][2]\n",
    "    col3.append(c)\n",
    "\n",
    "col4 = []\n",
    "for i in range(0, len(scores_pca)):\n",
    "    d = scores_pca[i][3]\n",
    "    col4.append(d)\n",
    "    \n",
    "col5 = []\n",
    "for i in range(0, len(scores_pca)):\n",
    "    e = scores_pca[i][4]\n",
    "    col5.append(e)\n",
    "\n",
    "col6 = []\n",
    "for i in range(0, len(scores_pca)):\n",
    "    f = kmeans_pca.labels_[i]\n",
    "    col6.append(f)\n",
    "\n",
    "#LIST OF PCA COLS AND KMEANS LABELS READY FOR DATA FRAME\n",
    "pca_dict = {\n",
    "    'PCA_1': col1,\n",
    "    'PCA_2': col2,\n",
    "    'PCA_3': col3,\n",
    "    'PCA_4': col4,\n",
    "    'PCA_5': col5,\n",
    "    'label': col6\n",
    "}\n",
    "pca_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABELLED PCA COLS IN DATA FRAME (PANDAS)\n",
    "pca_dict_df = pd.DataFrame(pca_dict)\n",
    "\n",
    "#READ IN PREVIOUS DATA TO APPLY LABELS BY MERGING\n",
    "panda_df = pd.read_csv(r'C:\\Users\\Cayo\\Downloads\\s5117135_Project_Code\\s5117135_Project_Code\\BOUN_TCP_Format.csv')\n",
    "\n",
    "#MERGE\n",
    "joined_panda = panda_df.join(pca_dict_df)\n",
    "\n",
    "#OUTPUT THE MERGE\n",
    "joined_panda.to_csv(r'C:\\Users\\Cayo\\Downloads\\s5117135_Project_Code\\s5117135_Project_Code\\BOUN_TCP_LABELS.csv', index=False)\n",
    "\n",
    "#READ INTO THE CSV FILE FROM THE MERGE\n",
    "Labelled_data = spark.read.csv(\n",
    "    path=r'C:\\Users\\Cayo\\Downloads\\s5117135_Project_Code\\s5117135_Project_Code\\BOUN_TCP_LABELS.csv',\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNT DATAPOINT IN EACH CLUSTER\n",
    "Labelled_data.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aebafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filters for value in column (Part of testing)\n",
    "Labelled_data.filter(col('Dst_ip').isin(['10.50.199.86']) == True).show(3)\n",
    "Labelled_data.filter(col('label').isin(['0']) == True).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts all the attack packets in each cluster\n",
    "IP_filter = Labelled_data.filter(col('Dst_ip').isin(['10.50.199.86']) == True)\n",
    "IP_filter0 = IP_filter.filter(col('label').isin([0]) == True).count()\n",
    "IP_filter1 = IP_filter.filter(col('label').isin([1]) == True).count()\n",
    "IP_filter2 = IP_filter.filter(col('label').isin([2]) == True).count()\n",
    "IP_filter3 = IP_filter.filter(col('label').isin([3]) == True).count()\n",
    "IP_filter4 = IP_filter.filter(col('label').isin([4]) == True).count()\n",
    "IP_filter5 = IP_filter.filter(col('label').isin([5]) == True).count()\n",
    "IP_Filter_Array = [IP_filter0,IP_filter1,IP_filter2,IP_filter3,IP_filter4,IP_filter5]\n",
    "IP_Filter_Array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
